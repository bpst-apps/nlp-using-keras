{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d770ed",
   "metadata": {},
   "source": [
    "## Text classification from scratch\n",
    "\n",
    "In this notebook we will learn how to do text classification starting from raw text (as a set of text files on disk). We demonstrate the workflow on the IMDB sentiment classification dataset (unprocessed version). We use the TextVectorization layer for word splitting & indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3826293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bea14d",
   "metadata": {},
   "source": [
    "#### Load the data: IMDB movie review sentiment classification\n",
    "\n",
    "Let's download the data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e368286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6f18d",
   "metadata": {},
   "source": [
    "The `aclImdb` folder contains a `train` and `test` subfolder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a56389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is A225-444F\n",
      "\n",
      " Directory of D:\\My-Work\\Git-Repo\\nlp-using-keras\\aclImdb\n",
      "\n",
      "26-06-2011  06:38    <DIR>          .\n",
      "07-07-2022  23:58    <DIR>          ..\n",
      "12-04-2011  22:44           845,980 imdb.vocab\n",
      "12-06-2011  04:24           903,029 imdbEr.txt\n",
      "26-06-2011  05:48             4,037 README\n",
      "12-04-2011  22:52    <DIR>          test\n",
      "26-06-2011  06:39    <DIR>          train\n",
      "               3 File(s)      1,753,046 bytes\n",
      "               4 Dir(s)  66,084,352,000 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c01fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is A225-444F\n",
      "\n",
      " Directory of D:\\My-Work\\Git-Repo\\nlp-using-keras\\aclImdb\\test\n",
      "\n",
      "12-04-2011  22:52    <DIR>          .\n",
      "26-06-2011  06:38    <DIR>          ..\n",
      "12-04-2011  22:55        20,205,283 labeledBow.feat\n",
      "12-04-2011  15:18    <DIR>          neg\n",
      "12-04-2011  15:18    <DIR>          pos\n",
      "12-04-2011  15:18           612,500 urls_neg.txt\n",
      "12-04-2011  15:18           612,500 urls_pos.txt\n",
      "               3 File(s)     21,430,283 bytes\n",
      "               4 Dir(s)  66,084,352,000 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir aclImdb\\test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f8a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is A225-444F\n",
      "\n",
      " Directory of D:\\My-Work\\Git-Repo\\nlp-using-keras\\aclImdb\\train\n",
      "\n",
      "26-06-2011  06:39    <DIR>          .\n",
      "26-06-2011  06:38    <DIR>          ..\n",
      "12-04-2011  22:47        21,021,197 labeledBow.feat\n",
      "12-04-2011  15:17    <DIR>          neg\n",
      "12-04-2011  15:17    <DIR>          pos\n",
      "12-04-2011  15:17    <DIR>          unsup\n",
      "12-04-2011  22:52        41,348,699 unsupBow.feat\n",
      "12-04-2011  15:18           612,500 urls_neg.txt\n",
      "12-04-2011  15:18           612,500 urls_pos.txt\n",
      "12-04-2011  15:17         2,450,000 urls_unsup.txt\n",
      "               5 File(s)     66,044,896 bytes\n",
      "               5 Dir(s)  66,084,352,000 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir aclImdb\\train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9d2b2",
   "metadata": {},
   "source": [
    "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of which represents one review (either positive or negative):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23729c7",
   "metadata": {},
   "source": [
    "We can use the utility `tf.keras.preprocessing.text_dataset_from_directory` to generate a labeled `tf.data.Dataset` object from a set of text files on disk filed into class-specific folders.\n",
    "\n",
    "Let's use it to generate the training, validation, and test datasets. The validation and training datasets are generated from two subsets of the train directory, with 20% of samples going to the validation dataset and 80% going to the training dataset.\n",
    "\n",
    "Having a validation dataset in addition to the test dataset is useful for tuning hyperparameters, such as the model architecture, for which the test dataset should not be used.\n",
    "\n",
    "Before putting the model out into the real world however, it should be retrained using all available training data (without creating a validation dataset), so its performance is maximized.\n",
    "\n",
    "When using the validation_split & subset arguments, make sure to either specify a random seed, or to pass shuffle=False, so that the validation & training splits you get have no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860379b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 3 classes.\n",
      "Using 15000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 1875\n",
      "Number of batches in raw_val_ds: 469\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133b777",
   "metadata": {},
   "source": [
    "Let's preview a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90e42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'SPOILERS: We sit through ten minutes of AWFUL clich\\xc3\\xa9d dialog at the beginning from two completely unoriginal characters with bad twangs (ripped off from Kalifornia and Natural Born Killers - there isn\\'t an original thing about these two) and you\\'re going \"either they\\'re about to kill everyone in the diner or already have\" and lo and behold guess what happens.<br /><br />I can\\'t stand all the Tarantino wannabes out there and this guy is one of the worst. I got maybe 25-30 minutes into the thing when I just couldn\\'t take it and stopped watching. Miner\\'s really bad acting was unbearable - I couldn\\'t take it. That, and the terrible script. After reading some of these comments I see there was a big twist - well guess what? No one cares. When you create completely uninteresting, unoriginal and unlikeable character like these two clich\\xc3\\xa9s, no one cares what big \"twist\" may have happened. I hope this is the end of these types of movies.'\n",
      "2\n",
      "b'This movie is horrible- in a \\'so bad it\\'s good\\' kind of way.<br /><br />The storyline is rehashed from so many other films of this kind, that I\\'m not going to even bother describing it. It\\'s a sword/sorcery picture, has a kid hoping to realize how important he is in this world, has a \"nomadic\" adventurer, an evil aide/sorcerer, a princess, a hairy creature....you get the point.<br /><br />The first time I caught this movie was during a very harsh winter. I don\\'t know why I decided to continue watching it for an extra five minutes before turning the channel, but when I caught site of Gulfax, I decided to stay and watch it until the end.<br /><br />Gulfax is a white, furry creature akin to Chewbacca, but not nearly as useful or entertaining to watch. He looks like someone glued a bunch of white shag carpeting together and forced the actor to wear it. There are scenes where it looks like the actor cannot move within, or that he\\'s almost falling over. Although he isn\\'t in the movie that much, the few scenes are worth it! Watch as he attempts to talk smack to Bo Svenson, taking the Solo-Chewbacca comparison\\'s to an even higher level! <br /><br />I actually bought this movie just because of that character, and still have it somewhere! <br /><br />Gulfax may look like sh!t, but he made this movie!!! The only reason I\\'ve never seen the sequel, or even sought it out, was because of his absence! Perhaps should there be a final film, completing the trilogy, Gulfax will make a much-anticipated return!'\n",
      "1\n",
      "b\"Pierce Brosnan has sipped his last Martini and returns, in an outrageous self-parody, as the aging foul-mouthed boozy assassin Julian Noble, who has a particular fondness for teenage girls, bullfights and tacky clothes. During a job in Mexico City he meets Danny (Greg Kinnear), a straight-faced Denver suburban business-man, who's in town to make his deal of-a-life-time, in a hotel bar. Despite their completely different personalities and Julian's crude and insensible remarks, they become friends. <br /><br />Largely carried by the performances of Pierce Brosnan and Greg Kinnear, director Richard Shepard revealed that he didn't write the film with Pierce Brosnan in mind , but I can hardly imagine this without him. He proves to have a real talent for comedy and can be more than just James Bond or cold-war spies. The scene in which the two meet at a glossy hotel bar (stunning sets and beautifully photographed) really is a bravura piece of acting skills. The scene lasts almost fifteen minutes, and although it was probably carefully scripted, the two actors are largely improvising, but they succeed wonderfully! It almost feels like a new standard in screen acting. Think of Robert De Niro and Harvey Keitel in MEAN STEETS improvising and add one of the most subtle underpinnings of many genre clich\\xc3\\xa9s and the actors' own typecasting (Brosnan's James Bond in particular), and you got one of the most delightful pairings in recent Hollywood. <br /><br />Sadly, the story wears thin after a while. After an hour, the film just runs out of steam. Nevertheless, and I can't put my finger on it exactly, I did enjoy this very much. It just feels very fresh and original, with some imaginative use of sets and lighting, and some hints to Seijun Suzuki and Jean-Pierre Melville. The other characters aren't given much to do, but this film does offer something new, in that respect it almost effortlessly succeeds in blending all conventional genres into quite an entertaining spoof. Very amusing.<br /><br />Camera Obscura --- 7/10\"\n",
      "1\n",
      "b\"I hadn't seen this show until it was repeated on Dave. I became aware of it after seeing The Comedy Store Players, a group of comedians which includes Josie Lawrence.<br /><br />I enjoy the unpredictability of Whose Line is it Anyway and the diversity of the performers. The fact that the show had both English and American comedians made it better.<br /><br />The best performance ever had to be when Josie Lawrence and Caroline Quentin sung a duet about a beached whale. Admitedly that description may not sound funny, but if you've seen it you'll know how good it was.<br /><br />Colin Mochrie and Ryan Stiles were a great double act. They were particularly good at the round where they were given two completely random lines to incorporate into a sketch.<br /><br />Other improv shows such as Mock The Week are good, but Whose Line is it Anyway will always be the king of Improvised Comedy shows\"\n",
      "2\n",
      "b'This film, directed by New World Pictures alumnus Jonathan Kaplan and starring cult favorite Aimee Graham, is part of the \"Rebel Highway\" series of remakes of 1950s juvenile-delinquent films.<br /><br />For what was basically a chance for the filmmakers to have fun, _Reform School Girl_ is quite watchable. There are the allusions to McCarthyism characteristic of the \"Rebel Highway\" series (and a well-done general 50s ambiance), and the usual array of interesting types we meet in women-in-prison films. This one is nowhere near as graphic as a typical entry in that genre, but there is one lesbian love scene that is strikingly filmed and acted, suprisingly graphic for such young-looking actresses, and really kind of a tonal shift from the rest of the film. Unfortunately, the film never really resolves the lesbian relationship.<br /><br />The only time I really cringed was the scene where Donna (Aimee Graham) started dancing and singing with the mop, and the camera began moving around and getting in the faces of the onlookers. This was a totally ridiculous scene (but I guess it\\'s hard to pad these things out to 80 minutes).<br /><br />Graham is stellar in the title role, a girl who has had to deal with abuse and, while in reform school, awakens to more positive sexual experiences. I really wish Hollywood would take more notice of her.'\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "# This is one of the places where eager execution shines:\n",
    "# we can just evaluate these tensors using .numpy()\n",
    "# instead of needing to evaluate them in a Session/Graph context.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8372b1",
   "metadata": {},
   "source": [
    "#### Prepare the data\n",
    "\n",
    "In particular, we remove `<br />` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af57cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9214ecd",
   "metadata": {},
   "source": [
    "### Two options to vectorize the data\n",
    "\n",
    "There are 2 ways we can use our text vectorization layer:\n",
    "\n",
    "**Option 1:** Make it part of the model, so as to obtain a model that processes raw strings, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dcc69",
   "metadata": {},
   "source": [
    "```python\n",
    "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "x = vectorize_layer(text_input)\n",
    "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "...\n",
    "```\n",
    "\n",
    "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
    " feed it into a model that expects integer sequences as inputs.\n",
    "\n",
    "An important difference between the two is that option 2 enables you to do\n",
    "**asynchronous CPU processing and buffering** of your data when training on GPU.\n",
    "So if you're training the model on GPU, you probably want to go with this option to get\n",
    " the best performance. This is what we will do below.\n",
    "\n",
    "If we were to export our model to production, we'd ship a model that accepts raw\n",
    "strings as input, like in the code snippet for option 1 above. This can be done after\n",
    " training. We do this in the last section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1409bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f185f0",
   "metadata": {},
   "source": [
    "### Build a model\n",
    "\n",
    "We choose a simple 1D convnet starting with an `Embedding` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328c9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756f0a4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5fc5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 27s 12ms/step - loss: -396317884416.0000 - accuracy: 0.1664 - val_loss: -2115824779264.0000 - val_accuracy: 0.1677\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -15165249552384.0000 - accuracy: 0.1664 - val_loss: -38134976872448.0000 - val_accuracy: 0.1677\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: -101723834155008.0000 - accuracy: 0.1664 - val_loss: -184929564491776.0000 - val_accuracy: 0.1677\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: -358499846455296.0000 - accuracy: 0.1664 - val_loss: -554601577185280.0000 - val_accuracy: 0.1677\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -918128582197248.0000 - accuracy: 0.1664 - val_loss: -1296689952456704.0000 - val_accuracy: 0.1677\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -1960723571802112.0000 - accuracy: 0.1664 - val_loss: -2607504441737216.0000 - val_accuracy: 0.1677\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -3703591369965568.0000 - accuracy: 0.1664 - val_loss: -4727378160910336.0000 - val_accuracy: 0.1677\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -6450018676375552.0000 - accuracy: 0.1664 - val_loss: -7955829378514944.0000 - val_accuracy: 0.1677\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -10526649162399744.0000 - accuracy: 0.1664 - val_loss: -12648524067897344.0000 - val_accuracy: 0.1677\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: -16313285443846144.0000 - accuracy: 0.1664 - val_loss: -19209928426127360.0000 - val_accuracy: 0.1677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1daaf1fb820>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1293e",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c243a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 19424008923512832.0000 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.942400892351283e+16, 0.5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e256dd",
   "metadata": {},
   "source": [
    "### Make an end-to-end model\n",
    "\n",
    "If we want to obtain a model capable of processing raw strings, we can simply create a new model (using the weights we just trained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e450f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 8s 10ms/step - loss: 19424008923512832.0000 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.942400892351283e+16, 0.5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
